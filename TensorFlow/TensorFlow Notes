Data types:
-training (most)
-testing
-validation (least)


//-----------------------------------------------------------------------------
A tensor is a typed multi-dimensional array.
//-----------------------------------------------------------------------------

//-----------------------------------------------------------------------------
x = tf.placeholder(tf.float32, [None, 784])

x is NOT a specific value. It is a placeholder, a value that we will use as
input when we ask TensorFlow to run a computation.

placeholder(dataType, shape)

example: tf.placeholder(tf.float32, [None, 784])
tf.float32 makes this a float array
Where it says "None", that is how many data points there are
Where it says "784", that is the number of values per data point

Example:
We have 100 images of dogs and cats. Each image consists of 28x28 pixels
(28x28=784). x[0] would be image 1, x[99] would be image 100. Having None means 
that if we found some new images, we can add them to our data set without 
changing this value.
//-----------------------------------------------------------------------------

//-----------------------------------------------------------------------------
W = tf.Variable(tf.zeros[784, 10])

W is a variable. That means that W is a modifiable tensor that lives in TensorFlow's graph of interacting operations. Thi can be used and modified by the computations.
//-----------------------------------------------------------------------------

//-----------------------------------------------------------------------------
y = tf.nn.softmax(tf.matmul(x, W) + b)

The above line implements the model.
First, x is multiplied by W with "tf.matmul(x, W)". Why is x multiplied by W?
Remember that x is a 2D tensor! x is NOT a single data point.
Second, the bias b is added
Finally, the softmax function is applied.
//-----------------------------------------------------------------------------

//-----------------------------------------------------------------------------
softmax takes values and creates probabilities that add up to 1. With an output
layer of 4 neurons, one might be .8, another .11, and the last two both .045
//-----------------------------------------------------------------------------

//-----------------------------------------------------------------------------
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

tf.log computes the logarithm of EACH element of y
Next, multiply EACH element of y_ with the corresponding element of tf.log(y)
Third, tf.reduce_sum adds up the elements in the second dimension of y because
we have reduction_indices=[1]
Finally, tf.reduce_mean computes the mean over ALL the examples in the batch
//-----------------------------------------------------------------------------

//-----------------------------------------------------------------------------  cross_entropy = tf.reduce_mean(      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))

The tf.nn.softmax_cross_entropy_with_logits is more numerically stable than
implementing the cross entropy ourselves as done above.
//-----------------------------------------------------------------------------

//-----------------------------------------------------------------------------
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

This asks TensorFlow to use gradient descent with a learning rate of 0.5 to
get the weights to their "correct" values.
//-----------------------------------------------------------------------------
